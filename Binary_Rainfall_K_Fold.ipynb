{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdPiaxT1dvHg4zxtronr5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fxs2596/NerdOut/blob/main/Binary_Rainfall_K_Fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Complete Machine Learning Workflow Script (Start to Finish - RF Group CV Tuning) ---\n",
        "\n",
        "# --- Step 0: Setup and Data Loading ---\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Import necessary scikit-learn modules\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GroupKFold # Import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Import Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"Setup complete. Importing libraries.\")\n",
        "\n",
        "# Define target column name early\n",
        "target_col = 'rainfall' # Correct target column name\n",
        "\n",
        "# Load the training dataset\n",
        "file_name = 'train.csv' # Correct file name\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print(f\"\\nData '{file_name}' loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: File '{file_name}' not found. Please check the file name and path.\")\n",
        "    import sys\n",
        "    sys.exit(f\"Data file '{file_name}' not found.\")\n",
        "\n",
        "print(f\"Target variable defined as '{target_col}'.\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of the loaded data:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# --- Step 1: Data Inspection and Cleaning (Initial & Group Creation) ---\n",
        "print(\"\\n--- Step 1: Data Inspection and Cleaning (Initial & Group Creation) ---\")\n",
        "\n",
        "print(\"\\nColumn Info (data types and non-null counts):\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nSummary Statistics for Numerical Columns:\")\n",
        "df.describe()\n",
        "\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "df.isnull().sum()\n",
        "\n",
        "print(f\"\\nDistribution of the target variable '{target_col}':\")\n",
        "df[target_col].value_counts()\n",
        "\n",
        "print(f\"\\nPercentage Distribution of the target variable '{target_col}':\")\n",
        "df[target_col].value_counts(normalize=True) * 100\n",
        "\n",
        "# 1. Handle Data Quality Issues: rain > related columns? (Not applicable for binary target)\n",
        "#    Handle rows where target > length - This was for regression, skip here\n",
        "\n",
        "# Create the 'group' column *before* dropping 'id' or filtering rows\n",
        "# Based on winner strategy: group = id // 365\n",
        "if 'id' in df.columns:\n",
        "    df['group'] = df['id'] // 365\n",
        "    print(\"\\nCreated 'group' column based on id // 365.\")\n",
        "    print(\"Unique groups and counts:\")\n",
        "    print(df['group'].value_counts().sort_index())\n",
        "else:\n",
        "    print(\"\\nWarning: 'id' column not found. Cannot create 'group' column for Group K-Fold.\")\n",
        "    # If 'id' is missing, we cannot use GroupKFold as winner did. Fallback might be needed.\n",
        "    # For now, assume 'id' exists and group is created.\n",
        "\n",
        "\n",
        "# Drop columns not needed for modeling\n",
        "columns_to_drop = ['id', 'day'] # Dropping 'id' and 'day'\n",
        "\n",
        "df_cleaned = df.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"\\nDropped columns: {columns_to_drop}\")\n",
        "print(f\"Shape after dropping columns: {df_cleaned.shape}\")\n",
        "print(\"First 5 rows after dropping columns:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "\n",
        "# --- Step 2: EDA (Correlation Analysis - Numerical Features) ---\n",
        "print(\"\\n--- Step 2: EDA (Correlation Analysis - Numerical Features) ---\")\n",
        "\n",
        "# Identify numerical columns after dropping, BEFORE separating groups\n",
        "numerical_cols_cleaned = df_cleaned.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "print(\"\\nCorrelation Matrix for Numerical Features:\")\n",
        "correlation_matrix = df_cleaned[numerical_cols_cleaned].corr()\n",
        "# print(correlation_matrix) # Commented out\n",
        "\n",
        "print(\"\\nCorrelation with the Target Variable ('rainfall'):\")\n",
        "if target_col in df_cleaned.columns:\n",
        "    correlation_with_target = df_cleaned[numerical_cols_cleaned].corr()[target_col].sort_values(ascending=False)\n",
        "    print(correlation_with_target)\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_col}' not found after dropping columns.\")\n",
        "\n",
        "\n",
        "# --- Step 2: EDA (Categorical Feature vs. Target) ---\n",
        "print(\"\\n--- Step 2: EDA (Categorical Feature vs. Target) ---\")\n",
        "\n",
        "categorical_column_name = 'winddirection'\n",
        "\n",
        "print(f\"\\nUnique values and data type for '{categorical_column_name}':\")\n",
        "if categorical_column_name in df_cleaned.columns:\n",
        "    print(df_cleaned[categorical_column_name].value_counts())\n",
        "    print(df_cleaned[categorical_column_name].dtype)\n",
        "else:\n",
        "     print(f\"Error: '{categorical_column_name}' not found in cleaned data.\")\n",
        "\n",
        "print(f\"\\nRelationship between '{categorical_column_name}' and '{target_col}':\")\n",
        "if categorical_column_name in df_cleaned.columns and target_col in df_cleaned.columns:\n",
        "    rainfall_by_category = df_cleaned.groupby(categorical_column_name)[target_col].mean().sort_values(ascending=False)\n",
        "    print(rainfall_by_category)\n",
        "else:\n",
        "    print(f\"Error: '{categorical_column_name}' or target column not found in cleaned data.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Data Preprocessing ---\n",
        "print(\"\\n--- Step 3: Data Preprocessing ---\")\n",
        "\n",
        "# Define features (X), target (y), and groups from the cleaned DataFrame\n",
        "# The 'group' column should NOT be in X, but kept separate for the split\n",
        "X = df_cleaned.drop(columns=[target_col, 'group']) # Drop target and group from features\n",
        "y = df_cleaned[target_col]\n",
        "groups = df_cleaned['group'] # Keep the groups series separately\n",
        "\n",
        "\n",
        "print(f\"\\nFeatures shape (X): {X.shape}\")\n",
        "print(f\"Target shape (y): {y.shape}\")\n",
        "print(f\"Groups shape: {groups.shape}\")\n",
        "\n",
        "\n",
        "# ** IMPORTANT: Split data *NOW* after basic cleaning but before feature transformations **\n",
        "# When using GroupKFold for CV later, we need the original groups aligned with X_train.\n",
        "# We still need a single overall train/test split for final evaluation (Step 8).\n",
        "# Let's do the overall train/test split here, making sure to split the groups as well.\n",
        "print(\"\\n--- Splitting Data into Overall Training and Testing Sets (with Groups) ---\")\n",
        "\n",
        "# Split X, y, and groups into train and test sets\n",
        "# Using stratify=y for target split consistency, although GroupKFold handles group consistency\n",
        "# We pass groups to train_test_split to keep them aligned with the X, y splits\n",
        "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
        "    X, y, groups, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set shape (X_train, y_train, groups_train): {X_train.shape}, {y_train.shape}, {groups_train.shape}\")\n",
        "print(f\"Test set shape (X_test, y_test, groups_test): {X_test.shape}, {y_test.shape}, {groups_test.shape}\")\n",
        "\n",
        "\n",
        "# --- Identify Column Types for Preprocessing (based on X_train) ---\n",
        "# Numerical columns: Select number types, excluding the target and group (already separated)\n",
        "numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Categorical columns for OHE: Based on our analysis, winddirection\n",
        "categorical_cols = [categorical_column_name] # Use the defined categorical column name\n",
        "\n",
        "# Remove categorical from numerical_cols list if it somehow got in there initially\n",
        "if categorical_column_name in numerical_cols:\n",
        "    numerical_cols.remove(categorical_column_name)\n",
        "\n",
        "\n",
        "print(f\"\\nFeatures identified for transformation:\")\n",
        "print(f\"Numerical: {numerical_cols}\")\n",
        "print(f\"Categorical (for OHE): {categorical_cols}\")\n",
        "\n",
        "\n",
        "# --- Create Preprocessing Pipelines/Objects (Fitted on Training Data) ---\n",
        "print(\"\\n--- Creating and Fitting Preprocessing Objects (on Training Data) ---\")\n",
        "\n",
        "# 1. Handle Missing Values (Numerical Columns) - Using Median (Redundant here as no missing data)\n",
        "# Fit imputer on training data ONLY\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "imputer.fit(X_train[numerical_cols])\n",
        "print(\"Numerical imputer fitted on training data (no missing data found, so it won't change values).\")\n",
        "\n",
        "# 2. Encode Categorical Features (Fit on Training Data)\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoder.fit(X_train[categorical_cols]) # Fit encoder on training data only on specified categorical columns\n",
        "encoded_feature_names = encoder.get_feature_names_out(categorical_cols) # Get the names of the new columns\n",
        "print(f\"Categorical encoder fitted on training data for {categorical_cols}.\")\n",
        "print(f\"  Encoder learned {len(encoded_feature_names)} new features.\")\n",
        "\n",
        "# 3. Scale Numerical Features (Fit on Training Data)\n",
        "scaler = StandardScaler()\n",
        "# Fit scaler on training data ONLY (using the potentially imputed data, though none was imputed)\n",
        "scaler.fit(X_train[numerical_cols])\n",
        "print(\"Numerical scaler fitted on training data.\")\n",
        "\n",
        "\n",
        "# --- Apply Preprocessing Transformations ---\n",
        "print(\"\\n--- Applying Preprocessing Transformations ---\")\n",
        "\n",
        "# --- Apply to Training Data ---\n",
        "# Apply Imputation (no effect here)\n",
        "X_train_imputed = pd.DataFrame(imputer.transform(X_train[numerical_cols]), columns=numerical_cols, index=X_train.index)\n",
        "\n",
        "# Apply Scaling\n",
        "X_train_scaled = scaler.transform(X_train_imputed)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)\n",
        "print(f\"Training numerical features scaled. Shape: {X_train_scaled_df.shape}\")\n",
        "\n",
        "# Apply Encoding\n",
        "X_train_encoded = encoder.transform(X_train[categorical_cols]) # Transform original (not imputed/scaled) categorical\n",
        "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
        "print(f\"Training categorical features encoded. Shape: {X_train_encoded_df.shape}\")\n",
        "\n",
        "# Combine scaled numerical and encoded categorical features for training\n",
        "X_train_processed = pd.concat([X_train_scaled_df, X_train_encoded_df], axis=1)\n",
        "print(f\"\\nFinal processed TRAIN features shape: {X_train_processed.shape}\")\n",
        "print(\"Final processed TRAIN columns (first 10):\", X_train_processed.columns.tolist()[:10])\n",
        "\n",
        "\n",
        "# --- Apply to Testing Data ---\n",
        "# Apply Imputation using the *FITTED* imputer from training\n",
        "X_test_imputed = pd.DataFrame(imputer.transform(X_test[numerical_cols]), columns=numerical_cols, index=X_test.index)\n",
        "\n",
        "# Apply Scaling using the *FITTED* scaler from training\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)\n",
        "print(f\"\\nTesting numerical features scaled. Shape: {X_test_scaled_df.shape}\")\n",
        "\n",
        "# Apply Encoding using the *FITTED* encoder from training\n",
        "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
        "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
        "print(f\"Testing categorical features encoded. Shape: {X_test_encoded_df.shape}\")\n",
        "\n",
        "# Combine scaled numerical and encoded categorical features for testing\n",
        "X_test_processed = pd.concat([X_test_scaled_df, X_test_encoded_df], axis=1)\n",
        "\n",
        "# **CRUCIAL**: Reindex test columns to match train columns exactly after processing\n",
        "# Ensure X_train_processed exists from Step 3\n",
        "if 'X_train_processed' in locals():\n",
        "    X_test_processed = X_test_processed[X_train_processed.columns]\n",
        "    print(\"Submission test columns reordered to match training columns.\")\n",
        "else:\n",
        "     # This case should ideally not happen if running the script sequentially\n",
        "     print(\"Warning: X_train_processed not found during test processing.\")\n",
        "\n",
        "\n",
        "print(f\"Final processed TEST features shape: {X_test_processed.shape}\")\n",
        "print(\"Final processed TEST columns (first 10):\", X_test_processed.columns.tolist()[:10])\n",
        "\n",
        "print(\"\\nData Preprocessing Complete!\")\n",
        "\n",
        "\n",
        "# --- Step 4: Model Selection (Starting Group CV Tuning for RF) ---\n",
        "print(\"\\n--- Step 4: Model Selection (Starting Group CV Tuning for RF) ---\")\n",
        "print(\"Starting Group K-Fold Cross-Validation tuning for Random Forest Classifier.\")\n",
        "\n",
        "\n",
        "# --- Step 5 & 6: (Previous evaluations are implicitly part of our analysis) ---\n",
        "\n",
        "\n",
        "# --- Step 7: Hyperparameter Tuning (Randomized Search CV for Random Forest Classifier - Group K-Fold) ---\n",
        "print(\"\\n--- Step 7: Hyperparameter Tuning (Randomized Search CV for Random Forest Classifier - Group K-Fold) ---\")\n",
        "\n",
        "# param_distributions_rf already defined above\n",
        "\n",
        "print(\"Initializing base Random Forest Classifier model for tuning...\")\n",
        "rf_base_model_for_tuning = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Initialize GroupKFold CV\n",
        "# Winner used 6 folds\n",
        "n_splits_gkfold = 6\n",
        "print(f\"Initializing GroupKFold with {n_splits_gkfold} splits.\")\n",
        "gkfold = GroupKFold(n_splits=n_splits_gkfold)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "print(\"Initializing RandomizedSearchCV for Random Forest tuning with GroupKFold...\")\n",
        "random_search_rf_gkfold = RandomizedSearchCV(\n",
        "    estimator=rf_base_model_for_tuning,\n",
        "    param_distributions=param_distributions_rf, # Using the same RF parameter space\n",
        "    n_iter=10, # <-- Number of iterations/combinations to try (adjust as needed for time vs thoroughness)\n",
        "    cv=gkfold, # Use the GroupKFold object here\n",
        "    scoring='roc_auc', # Optimize for ROC AUC\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV using the processed training data AND the training groups\n",
        "print(f\"Starting Randomized Search Group K-Fold Cross-Validation for Random Forest with {random_search_rf_gkfold.n_iter} iterations and {gkfold.get_n_splits()} folds ({random_search_rf_gkfold.n_iter * gkfold.get_n_splits()} total fits)...\")\n",
        "print(\"This tuning respects the 'year' grouping to prevent leakage.\")\n",
        "start_time_rf_tune_gkfold = time.time()\n",
        "\n",
        "random_search_rf_gkfold.fit(X_train_processed, y_train, groups=groups_train) # Pass groups_train here!\n",
        "\n",
        "end_time_rf_tune_gkfold = time.time()\n",
        "print(\"\\nRandomized Search Group K-Fold Cross-Validation for Random Forest complete.\")\n",
        "print(f\"Random Forest Group CV Tuning took {end_time_rf_tune_gkfold - start_time_rf_tune_gkfold:.4f} seconds.\")\n",
        "\n",
        "\n",
        "# Get the best hyperparameters and the best score found from Group CV\n",
        "best_params_rf_gkfold = random_search_rf_gkfold.best_params_\n",
        "best_cv_score_rf_gkfold = random_search_rf_gkfold.best_score_ # This is the mean Group K-Fold ROC AUC\n",
        "\n",
        "print(\"\\n--- Randomized Search Results for Random Forest (Group K-Fold Tuning) ---\")\n",
        "print(\"Best Hyperparameters found:\", best_params_rf_gkfold)\n",
        "print(f\"Best Cross-Validated ROC AUC (Group K-Fold): {best_cv_score_rf_gkfold:.4f}\")\n",
        "\n",
        "# The best model found by the Group CV search\n",
        "best_rf_model_gkfold = random_search_rf_gkfold.best_estimator_\n",
        "\n",
        "print(\"\\nBest tuned (Group K-Fold) Random Forest estimator is ready.\")\n",
        "\n",
        "\n",
        "# --- Step 8: Final Evaluation of the Tuned Model on the Hold-Out Test Set ---\n",
        "print(\"\\n--- Step 8: Final Evaluation of Tuned Random Forest Model on Hold-Out Test Set ---\")\n",
        "\n",
        "# Use the best tuned model (best_rf_model_gkfold) found in Step 7 (Group CV)\n",
        "print(\"Evaluating the best tuned Random Forest model (from Group CV) on the UNSEEN hold-out test set...\")\n",
        "\n",
        "# Make predictions on the test set (X_test_processed which was NOT used in tuning CV folds)\n",
        "y_pred_tuned_rf_test_gkfold = best_rf_model_gkfold.predict(X_test_processed)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob_tuned_rf_test_gkfold = best_rf_model_gkfold.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "print(\"Predictions complete. Calculating metrics...\")\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_tuned_rf_gkfold = accuracy_score(y_test, y_pred_tuned_rf_test_gkfold)\n",
        "precision_tuned_rf_gkfold = precision_score(y_test, y_pred_tuned_rf_test_gkfold)\n",
        "recall_tuned_rf_gkfold = recall_score(y_test, y_pred_tuned_rf_test_gkfold)\n",
        "f1_tuned_rf_gkfold = f1_score(y_test, y_pred_tuned_rf_test_gkfold)\n",
        "roc_auc_tuned_rf_gkfold = roc_auc_score(y_test, y_prob_tuned_rf_test_gkfold)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"\\nTuned Random Forest Classifier Model Evaluation on Hold-Out Test Set (from Group CV Tuning):\")\n",
        "print(f\"Accuracy: {accuracy_tuned_rf_gkfold:.4f}\")\n",
        "print(f\"Precision: {precision_tuned_rf_gkfold:.4f}\")\n",
        "print(f\"Recall: {recall_tuned_rf_gkfold:.4f}\")\n",
        "print(f\"F1-Score: {f1_tuned_rf_gkfold:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_tuned_rf_gkfold:.4f}\")\n",
        "\n",
        "print(\"\\nFinal evaluation of tuned (Group K-Fold) Random Forest model complete.\")\n",
        "\n",
        "\n",
        "# --- Step 9: Prediction on New Data / Submission ---\n",
        "print(\"\\n--- Step 9: Prediction on New Data / Submission ---\")\n",
        "\n",
        "# Load the new test dataset for submission\n",
        "submission_file_name_input = 'test.csv'\n",
        "\n",
        "try:\n",
        "    df_submission_test = pd.read_csv(submission_file_name_input)\n",
        "    print(f\"\\nSubmission test data '{submission_file_name_input}' loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: '{submission_file_name_input}' not found. Cannot generate submission file.\")\n",
        "    # Assuming this is run in an environment where test.csv MUST be present for submission\n",
        "    # import sys\n",
        "    # sys.exit(f\"Submission data file '{submission_file_name_input}' not found.\")\n",
        "    # If running conceptually without test.csv, add a flag or handle gracefully\n",
        "    df_submission_test = None # Set to None if file not found to avoid further errors\n",
        "\n",
        "\n",
        "if df_submission_test is not None and not df_submission_test.empty:\n",
        "    print(\"\\nSubmission Test Data Inspection:\")\n",
        "    print(df_submission_test.head())\n",
        "    print(df_submission_test.info())\n",
        "\n",
        "    # ** IMPORTANT: Apply the *SAME* preprocessing steps using the *FITTED* objects from training **\n",
        "    # Use the imputer, encoder, scaler objects fitted in Step 3 on the training data.\n",
        "    # Use the lists of columns (numerical_cols, categorical_cols, encoded_feature_names) from Step 3.\n",
        "\n",
        "    df_submission_processed = df_submission_test.copy()\n",
        "\n",
        "    # Store submission IDs before preprocessing\n",
        "    submission_ids = df_submission_processed['id']\n",
        "\n",
        "    # Drop 'id' and 'day' from the submission features, similar to training\n",
        "    submission_feature_cols = [col for col in df_submission_processed.columns if col not in ['id', 'day']]\n",
        "    df_submission_features = df_submission_processed[submission_feature_cols]\n",
        "\n",
        "    # numerical_cols and categorical_cols and encoded_feature_names are available from Step 3\n",
        "\n",
        "\n",
        "    # --- Apply Missing Value Imputation using the *FITTED* Imputer ---\n",
        "    print(\"\\n--- Handling Missing Values in Submission Test Data ---\")\n",
        "    # Use the imputer fitted on training data's numerical_cols\n",
        "    X_submission_numerical_imputed = imputer.transform(df_submission_features[numerical_cols])\n",
        "    X_submission_numerical_imputed_df = pd.DataFrame(X_submission_numerical_imputed, columns=numerical_cols, index=df_submission_features.index)\n",
        "    print(\"Missing values imputed using imputer fitted on training data.\")\n",
        "\n",
        "\n",
        "    # --- Apply Categorical Encoding using the *FITTED* Encoder ---\n",
        "    print(\"\\n--- Encoding Categorical Features in Submission Test Data ---\")\n",
        "    # Use the encoder fitted on training data's categorical_cols\n",
        "    X_submission_encoded = encoder.transform(df_submission_features[categorical_cols])\n",
        "    # Use the feature names learned from training\n",
        "    X_submission_encoded_df = pd.DataFrame(X_submission_encoded, columns=encoded_feature_names, index=df_submission_features.index)\n",
        "    print(\"Categorical features encoded using encoder fitted on training data.\")\n",
        "    print(f\"Shape after One-Hot Encoding (Submission Test): {X_submission_encoded_df.shape}\")\n",
        "\n",
        "\n",
        "    # --- Apply Numerical Scaling using the *FITTED* Scaler ---\n",
        "    print(\"\\n--- Scaling Numerical Features in Submission Test Data ---\")\n",
        "    # Use the scaler fitted on training data's numerical_cols (after imputation in training)\n",
        "    X_submission_scaled = scaler.transform(X_submission_numerical_imputed_df[numerical_cols])\n",
        "    X_submission_scaled_df = pd.DataFrame(X_submission_scaled, columns=numerical_cols, index=df_submission_features.index)\n",
        "    print(\"Numerical features scaled using scaler fitted on training data.\")\n",
        "    print(f\"Shape after Scaling (Submission Test): {X_submission_scaled_df.shape}\")\n",
        "\n",
        "\n",
        "    # --- Combine Processed Features for Submission ---\n",
        "    print(\"\\n--- Combining Processed Submission Test Features ---\")\n",
        "\n",
        "    X_submission_processed = pd.concat([X_submission_scaled_df, X_submission_encoded_df], axis=1)\n",
        "    print(f\"Processed Submission Test features shape: {X_submission_processed.shape}\")\n",
        "\n",
        "    # **CRUCIAL**: Reindex submission features to match the column order of training features exactly\n",
        "    if 'X_train_processed' in locals():\n",
        "        X_submission_processed = X_submission_processed[X_train_processed.columns]\n",
        "        print(\"Submission test columns reordered to match training columns.\")\n",
        "    else:\n",
        "         print(\"Warning: X_train_processed not found during submission processing. Cannot guarantee submission column order matches training.\")\n",
        "\n",
        "\n",
        "    # --- Make Final Predictions using the Best Model ---\n",
        "    print(\"\\n--- Making Final Predictions ---\")\n",
        "    # Use the best performing model found (best_rf_model_gkfold from Step 7)\n",
        "\n",
        "    if 'best_rf_model_gkfold' in locals() and hasattr(best_rf_model_gkfold, 'predict'):\n",
        "        print(f\"Using the best Group K-Fold tuned model ('best_rf_model_gkfold') to generate predictions.\")\n",
        "        submission_predictions = best_rf_model_gkfold.predict(X_submission_processed)\n",
        "        print(\"Predictions generated for submission.\")\n",
        "\n",
        "        # --- Create Submission File ---\n",
        "        print(\"\\n--- Creating Submission File ---\")\n",
        "\n",
        "        submission_df = pd.DataFrame({'id': submission_ids, target_col: submission_predictions})\n",
        "\n",
        "        # Define the submission file name\n",
        "        submission_file_name_output = 'rainfall_submission_rf_gkfold.csv'\n",
        "\n",
        "        # Save the submission file\n",
        "        submission_df.to_csv(submission_file_name_output, index=False)\n",
        "\n",
        "        print(f\"Submission file '{submission_file_name_output}' created successfully!\")\n",
        "        print(submission_df.head())\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Best tuned model object ('best_rf_model_gkfold') not found or not fitted. Ensure training/tuning completed.\")\n",
        "        print(\"Submission file not created.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    if df_submission_test is not None:\n",
        "         print(\"\\nSubmission file not created due to empty submission test data.\")\n",
        "    else:\n",
        "         print(\"\\nSubmission file not created because 'test.csv' was not found.\")\n",
        "\n",
        "\n",
        "# --- End of Script ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS5DOGMkft1a",
        "outputId": "fa262afa-481e-442f-eb6f-56a4c64d9abd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Importing libraries.\n",
            "\n",
            "Data 'train.csv' loaded successfully!\n",
            "Target variable defined as 'rainfall'.\n",
            "\n",
            "First 5 rows of the loaded data:\n",
            "   id  day  pressure  maxtemp  temparature  mintemp  dewpoint  humidity  \\\n",
            "0   0    1    1017.4     21.2         20.6     19.9      19.4      87.0   \n",
            "1   1    2    1019.5     16.2         16.9     15.8      15.4      95.0   \n",
            "2   2    3    1024.1     19.4         16.1     14.6       9.3      75.0   \n",
            "3   3    4    1013.4     18.1         17.8     16.9      16.8      95.0   \n",
            "4   4    5    1021.8     21.3         18.4     15.2       9.6      52.0   \n",
            "\n",
            "   cloud  sunshine  winddirection  windspeed  rainfall  \n",
            "0   88.0       1.1           60.0       17.2         1  \n",
            "1   91.0       0.0           50.0       21.9         1  \n",
            "2   47.0       8.3           70.0       18.1         1  \n",
            "3   95.0       0.0           60.0       35.6         1  \n",
            "4   45.0       3.6           40.0       24.8         0  \n",
            "\n",
            "--- Step 1: Data Inspection and Cleaning (Initial & Group Creation) ---\n",
            "\n",
            "Column Info (data types and non-null counts):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2190 entries, 0 to 2189\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   id             2190 non-null   int64  \n",
            " 1   day            2190 non-null   int64  \n",
            " 2   pressure       2190 non-null   float64\n",
            " 3   maxtemp        2190 non-null   float64\n",
            " 4   temparature    2190 non-null   float64\n",
            " 5   mintemp        2190 non-null   float64\n",
            " 6   dewpoint       2190 non-null   float64\n",
            " 7   humidity       2190 non-null   float64\n",
            " 8   cloud          2190 non-null   float64\n",
            " 9   sunshine       2190 non-null   float64\n",
            " 10  winddirection  2190 non-null   float64\n",
            " 11  windspeed      2190 non-null   float64\n",
            " 12  rainfall       2190 non-null   int64  \n",
            "dtypes: float64(10), int64(3)\n",
            "memory usage: 222.6 KB\n",
            "\n",
            "Summary Statistics for Numerical Columns:\n",
            "\n",
            "Missing Values per Column:\n",
            "\n",
            "Distribution of the target variable 'rainfall':\n",
            "\n",
            "Percentage Distribution of the target variable 'rainfall':\n",
            "\n",
            "Created 'group' column based on id // 365.\n",
            "Unique groups and counts:\n",
            "group\n",
            "0    365\n",
            "1    365\n",
            "2    365\n",
            "3    365\n",
            "4    365\n",
            "5    365\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dropped columns: ['id', 'day']\n",
            "Shape after dropping columns: (2190, 12)\n",
            "First 5 rows after dropping columns:\n",
            "   pressure  maxtemp  temparature  mintemp  dewpoint  humidity  cloud  \\\n",
            "0    1017.4     21.2         20.6     19.9      19.4      87.0   88.0   \n",
            "1    1019.5     16.2         16.9     15.8      15.4      95.0   91.0   \n",
            "2    1024.1     19.4         16.1     14.6       9.3      75.0   47.0   \n",
            "3    1013.4     18.1         17.8     16.9      16.8      95.0   95.0   \n",
            "4    1021.8     21.3         18.4     15.2       9.6      52.0   45.0   \n",
            "\n",
            "   sunshine  winddirection  windspeed  rainfall  group  \n",
            "0       1.1           60.0       17.2         1      0  \n",
            "1       0.0           50.0       21.9         1      0  \n",
            "2       8.3           70.0       18.1         1      0  \n",
            "3       0.0           60.0       35.6         1      0  \n",
            "4       3.6           40.0       24.8         0      0  \n",
            "\n",
            "--- Step 2: EDA (Correlation Analysis - Numerical Features) ---\n",
            "\n",
            "Correlation Matrix for Numerical Features:\n",
            "\n",
            "Correlation with the Target Variable ('rainfall'):\n",
            "rainfall         1.000000\n",
            "cloud            0.641191\n",
            "humidity         0.454213\n",
            "windspeed        0.111625\n",
            "dewpoint         0.081965\n",
            "group            0.034118\n",
            "winddirection   -0.006939\n",
            "mintemp         -0.026841\n",
            "temparature     -0.049660\n",
            "pressure        -0.049886\n",
            "maxtemp         -0.079304\n",
            "sunshine        -0.555287\n",
            "Name: rainfall, dtype: float64\n",
            "\n",
            "--- Step 2: EDA (Categorical Feature vs. Target) ---\n",
            "\n",
            "Unique values and data type for 'winddirection':\n",
            "winddirection\n",
            "70.0     273\n",
            "220.0    241\n",
            "20.0     238\n",
            "40.0     230\n",
            "50.0     199\n",
            "80.0     178\n",
            "60.0     152\n",
            "230.0    142\n",
            "30.0      70\n",
            "200.0     58\n",
            "190.0     50\n",
            "10.0      47\n",
            "240.0     43\n",
            "100.0     33\n",
            "180.0     31\n",
            "210.0     30\n",
            "90.0      28\n",
            "130.0     27\n",
            "110.0     20\n",
            "120.0     15\n",
            "170.0     14\n",
            "270.0     10\n",
            "290.0     10\n",
            "250.0      9\n",
            "280.0      9\n",
            "300.0      7\n",
            "140.0      7\n",
            "160.0      6\n",
            "150.0      4\n",
            "25.0       3\n",
            "260.0      2\n",
            "75.0       1\n",
            "15.0       1\n",
            "250.3      1\n",
            "65.0       1\n",
            "Name: count, dtype: int64\n",
            "float64\n",
            "\n",
            "Relationship between 'winddirection' and 'rainfall':\n",
            "winddirection\n",
            "15.0     1.000000\n",
            "65.0     1.000000\n",
            "25.0     1.000000\n",
            "250.3    1.000000\n",
            "160.0    1.000000\n",
            "75.0     1.000000\n",
            "80.0     0.859551\n",
            "130.0    0.814815\n",
            "50.0     0.814070\n",
            "180.0    0.806452\n",
            "70.0     0.802198\n",
            "40.0     0.795652\n",
            "100.0    0.787879\n",
            "190.0    0.780000\n",
            "250.0    0.777778\n",
            "210.0    0.766667\n",
            "230.0    0.760563\n",
            "30.0     0.757143\n",
            "150.0    0.750000\n",
            "200.0    0.741379\n",
            "220.0    0.734440\n",
            "120.0    0.733333\n",
            "300.0    0.714286\n",
            "90.0     0.714286\n",
            "140.0    0.714286\n",
            "290.0    0.700000\n",
            "270.0    0.700000\n",
            "60.0     0.697368\n",
            "110.0    0.650000\n",
            "20.0     0.647059\n",
            "240.0    0.627907\n",
            "280.0    0.555556\n",
            "10.0     0.553191\n",
            "170.0    0.500000\n",
            "260.0    0.500000\n",
            "Name: rainfall, dtype: float64\n",
            "\n",
            "--- Step 3: Data Preprocessing ---\n",
            "\n",
            "Features shape (X): (2190, 10)\n",
            "Target shape (y): (2190,)\n",
            "Groups shape: (2190,)\n",
            "\n",
            "--- Splitting Data into Overall Training and Testing Sets (with Groups) ---\n",
            "\n",
            "Train set shape (X_train, y_train, groups_train): (1752, 10), (1752,), (1752,)\n",
            "Test set shape (X_test, y_test, groups_test): (438, 10), (438,), (438,)\n",
            "\n",
            "Features identified for transformation:\n",
            "Numerical: ['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity', 'cloud', 'sunshine', 'windspeed']\n",
            "Categorical (for OHE): ['winddirection']\n",
            "\n",
            "--- Creating and Fitting Preprocessing Objects (on Training Data) ---\n",
            "Numerical imputer fitted on training data (no missing data found, so it won't change values).\n",
            "Categorical encoder fitted on training data for ['winddirection'].\n",
            "  Encoder learned 34 new features.\n",
            "Numerical scaler fitted on training data.\n",
            "\n",
            "--- Applying Preprocessing Transformations ---\n",
            "Training numerical features scaled. Shape: (1752, 9)\n",
            "Training categorical features encoded. Shape: (1752, 34)\n",
            "\n",
            "Final processed TRAIN features shape: (1752, 43)\n",
            "Final processed TRAIN columns (first 10): ['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity', 'cloud', 'sunshine', 'windspeed', 'winddirection_10.0']\n",
            "\n",
            "Testing numerical features scaled. Shape: (438, 9)\n",
            "Testing categorical features encoded. Shape: (438, 34)\n",
            "Submission test columns reordered to match training columns.\n",
            "Final processed TEST features shape: (438, 43)\n",
            "Final processed TEST columns (first 10): ['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity', 'cloud', 'sunshine', 'windspeed', 'winddirection_10.0']\n",
            "\n",
            "Data Preprocessing Complete!\n",
            "\n",
            "--- Step 4: Model Selection (Starting Group CV Tuning for RF) ---\n",
            "Starting Group K-Fold Cross-Validation tuning for Random Forest Classifier.\n",
            "\n",
            "--- Step 7: Hyperparameter Tuning (Randomized Search CV for Random Forest Classifier - Group K-Fold) ---\n",
            "Initializing base Random Forest Classifier model for tuning...\n",
            "Initializing GroupKFold with 6 splits.\n",
            "Initializing RandomizedSearchCV for Random Forest tuning with GroupKFold...\n",
            "Starting Randomized Search Group K-Fold Cross-Validation for Random Forest with 10 iterations and 6 folds (60 total fits)...\n",
            "This tuning respects the 'year' grouping to prevent leakage.\n",
            "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n",
            "\n",
            "Randomized Search Group K-Fold Cross-Validation for Random Forest complete.\n",
            "Random Forest Group CV Tuning took 267.2519 seconds.\n",
            "\n",
            "--- Randomized Search Results for Random Forest (Group K-Fold Tuning) ---\n",
            "Best Hyperparameters found: {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 1.0, 'max_depth': None, 'bootstrap': True}\n",
            "Best Cross-Validated ROC AUC (Group K-Fold): 0.8920\n",
            "\n",
            "Best tuned (Group K-Fold) Random Forest estimator is ready.\n",
            "\n",
            "--- Step 8: Final Evaluation of Tuned Random Forest Model on Hold-Out Test Set ---\n",
            "Evaluating the best tuned Random Forest model (from Group CV) on the UNSEEN hold-out test set...\n",
            "Predictions complete. Calculating metrics...\n",
            "\n",
            "Tuned Random Forest Classifier Model Evaluation on Hold-Out Test Set (from Group CV Tuning):\n",
            "Accuracy: 0.8699\n",
            "Precision: 0.8867\n",
            "Recall: 0.9485\n",
            "F1-Score: 0.9165\n",
            "ROC AUC: 0.8777\n",
            "\n",
            "Final evaluation of tuned (Group K-Fold) Random Forest model complete.\n",
            "\n",
            "--- Step 9: Prediction on New Data / Submission ---\n",
            "\n",
            "Submission test data 'test.csv' loaded successfully!\n",
            "\n",
            "Submission Test Data Inspection:\n",
            "     id  day  pressure  maxtemp  temparature  mintemp  dewpoint  humidity  \\\n",
            "0  2190    1    1019.5     17.5         15.8     12.7      14.9      96.0   \n",
            "1  2191    2    1016.5     17.5         16.5     15.8      15.1      97.0   \n",
            "2  2192    3    1023.9     11.2         10.4      9.4       8.9      86.0   \n",
            "3  2193    4    1022.9     20.6         17.3     15.2       9.5      75.0   \n",
            "4  2194    5    1022.2     16.1         13.8      6.4       4.3      68.0   \n",
            "\n",
            "   cloud  sunshine  winddirection  windspeed  \n",
            "0   99.0       0.0           50.0       24.3  \n",
            "1   99.0       0.0           50.0       35.3  \n",
            "2   96.0       0.0           40.0       16.9  \n",
            "3   45.0       7.1           20.0       50.6  \n",
            "4   49.0       9.2           20.0       19.4  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 730 entries, 0 to 729\n",
            "Data columns (total 12 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   id             730 non-null    int64  \n",
            " 1   day            730 non-null    int64  \n",
            " 2   pressure       730 non-null    float64\n",
            " 3   maxtemp        730 non-null    float64\n",
            " 4   temparature    730 non-null    float64\n",
            " 5   mintemp        730 non-null    float64\n",
            " 6   dewpoint       730 non-null    float64\n",
            " 7   humidity       730 non-null    float64\n",
            " 8   cloud          730 non-null    float64\n",
            " 9   sunshine       730 non-null    float64\n",
            " 10  winddirection  729 non-null    float64\n",
            " 11  windspeed      730 non-null    float64\n",
            "dtypes: float64(10), int64(2)\n",
            "memory usage: 68.6 KB\n",
            "None\n",
            "\n",
            "--- Handling Missing Values in Submission Test Data ---\n",
            "Missing values imputed using imputer fitted on training data.\n",
            "\n",
            "--- Encoding Categorical Features in Submission Test Data ---\n",
            "Categorical features encoded using encoder fitted on training data.\n",
            "Shape after One-Hot Encoding (Submission Test): (730, 34)\n",
            "\n",
            "--- Scaling Numerical Features in Submission Test Data ---\n",
            "Numerical features scaled using scaler fitted on training data.\n",
            "Shape after Scaling (Submission Test): (730, 9)\n",
            "\n",
            "--- Combining Processed Submission Test Features ---\n",
            "Processed Submission Test features shape: (730, 43)\n",
            "Submission test columns reordered to match training columns.\n",
            "\n",
            "--- Making Final Predictions ---\n",
            "Using the best Group K-Fold tuned model ('best_rf_model_gkfold') to generate predictions.\n",
            "Predictions generated for submission.\n",
            "\n",
            "--- Creating Submission File ---\n",
            "Submission file 'rainfall_submission_rf_gkfold.csv' created successfully!\n",
            "     id  rainfall\n",
            "0  2190         1\n",
            "1  2191         1\n",
            "2  2192         1\n",
            "3  2193         0\n",
            "4  2194         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4KgSfmKgeND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jh5KmNc6hpkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uS4IPaphjd5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkvCJME1knxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whfv3fDMkg2y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}